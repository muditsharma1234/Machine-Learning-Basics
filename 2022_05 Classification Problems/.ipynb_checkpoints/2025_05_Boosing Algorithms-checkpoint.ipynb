{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718ccc58",
   "metadata": {},
   "source": [
    "### Boosting - One more Ensemble Technique\n",
    "Just like random forest boosting is one of the techniques under category of ensemble machine learning algorithms, where you train multiple models and try to solve for the objective of optimizing the end outcome. \n",
    "\n",
    "In this article we will try to understand the basic premise of boosting, how it works and what are the differences between random forest and boosting. In the end we will try to run our ```German Credit Score``` data to understand performance of boosting. \n",
    "\n",
    "This will be a relatively more theoretical article against the articles I have written in the past, hope you like it. \n",
    "So let's get going. \n",
    "\n",
    "#### What is boosting\n",
    "If I have to define boosting in one line then it will be the most popular definition - *Boosting is a technique of combining multiple weak-classifiers into a single strong classifier*. Now multiple weak classifiers if deployed in parallel would never result in a good result, because probability of each-one of them committing the same mistake is pretty high and when you average out you would on average get the similar mistakes being done. \n",
    "\n",
    "Which essentially is a classic adage of data-science *Garbage in Garbage out*. \n",
    "\n",
    "Therefore boosting is deployed as serial processing where each *weak-classifier* operates on the mistake done by the previous classifier and tries to reduce it resulting in an overall better result.\n",
    "\n",
    "If you have come across OHM's law ever this is very similar to resisters being deployed in parallel vs resisters being deployed in serial. In parallel they have very limited impact on overall resistance. However, when deployed in serial they increase the resistance of the circuit significantly. \n",
    "\n",
    "<img src = \"Images/Resistance.jpeg\" width = 500>\n",
    "\n",
    "If we were to associate resistance with the accuracy of model and each of the above resisters were weak classifiers, then in case of parallel algorithm we will end up getting lower accuracy. However, in case of serial algorithm (Image 2) we get a better accuracy. *This is just an example to demonstrate this*\n",
    "\n",
    "#### Difference between Boosting and Random Forest:\n",
    "Before we go to the details of Boosting and how it exactly works, let us try to understand some of the subtle differences between Random Forest and Boosting. This will help us in understanding the implementation of the algorithm. \n",
    "\n",
    "<img src = \"Images/Difference bw RF and Boosting.png\" width = 1000>\n",
    "\n",
    "As you can see the implementation of Boosting done by implementing tree-stubs (Trees with one node and two leaves) in series. Whereas in case of Random Forest the implementation is done by implementing complete trees in parallel. In order to appreciate the working of the algorithm let us try to understand the same via an example.\n",
    "\n",
    "*Note: In this particular example we will use Adaboost as an example, there are form of Boosting like Gradient Boost, which I will try to cover in other article.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Implementation of Adaboost algorithm:\n",
    "We will use an example of heart-disease prediction data to understand the concept. The data looks like:\n",
    "<img src = \"Images/Initial_data.png\" width = 600>\n",
    "\n",
    "##### Initial Weight\n",
    "First all the observations are given equal weights as there are 8 observations each observation gets 1/8 as weight weight\n",
    "\n",
    "##### Initial Stump\n",
    "Then we identify the first stump of the tree, which is essentially the first classification feature. For this we follow following steps:\n",
    "1. Identify the variable which classifies the heard disease best\n",
    "2. We create a decision tree for each of the features\n",
    "3. Compute Gini Index for each classification and select the feature with the lowest Gini Index\n",
    "\n",
    "Gini Index is laid down below:\n",
    "<img src = \"Images/Step 2.jpeg\" width = 800>\n",
    "\n",
    "4. We see that Patient Weight is having the lowest Gini Index and therefore this should be used as initial stump. This could have done directly via computing Incorrect Classifications as well. \n",
    "\n",
    "##### Computing Say of the First Stump\n",
    "In order to compute the accuracy of the stump we try to identify number of classifications correctly done by the classification rule, this can be observed below:\n",
    "<img src = \"Images/Classification Accuracy.png\" width = 900>\n",
    "\n",
    "1. Formula to compute the Say of a Stub the formula is given by:\n",
    "$\\frac{1}{2}\\times log(\\large \\frac{1-Total Error}{Total Error})$\n",
    "2. Total Error in step 1 is 1/8 because each element has same weight\n",
    "3. Say is given by $\\frac{1}{2}\\times log(\\frac{(1-1/8)}{(1/8)})$ = 0.97\n",
    "\n",
    "##### Computing Revised Weights:\n",
    "Now we have one sample which is incorrectly classified then the weight of the Sample = Original Weight X $e^{Say}$\n",
    "1. We had one incorrectly classified sample and the weight of that sample now is $\\frac{1}{8} \\times e^{0.97}$ = 0.33071\n",
    "2. We also compute the weights for correctly classified weights which is given by $e^{-say}$ = $\\frac{1}{8}\\times e^{-0.97}$ = 0.04725\n",
    "3. We replace the original weights with the computed weight of Correctly Classified and Incorrectly Classified\n",
    "4. Compute the Normalized Weight which is given by : Individual Weights/ Sum of All Weights\n",
    "\n",
    "We can look at the steps as follows:\n",
    "<img src = \"Images/Step 3.png\" width = 500>\n",
    "\n",
    "##### Creation of New Dataset using the Weight:\n",
    "Now we start randomly selecting values, from the original dataset based on the weight of individual record. Now for 4th element the probability of selecting is 50% and remaining records can be selected with a probability of 0.5, means if we have to select 8 records:\n",
    "1. 4 Records will belong the 4th row (This again with a very high probability)\n",
    "2. 4 each will belong to remaining 7 rows (not always equally distributed but apart from the 4th record all the other records are pretty much random)\n",
    "3. Which means the new data set will have 4 records of 4th element and and 4 elements from any of the remaining 7rows.\n",
    "\n",
    "This is generally done via a random-number generator between 0 and 1 and basis the value extracted a particular record is extracted. The random number generator will look at the table something like below to extract records:\n",
    "<img src = \"Images/Record Extraction.png\" width = 900>\n",
    "\n",
    "Post selection we once again select the stump to be used:\n",
    "<img src = \"Images/Selections_Feature.png\" width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468bc84",
   "metadata": {},
   "source": [
    "In this case our next stump is again based on Weight. However, the cut-off value now becomes 166 and not 176. We will keep repeating step. One thing to note here is because the mis-classified record was given high weight in previous step, it got selected multiple times.\n",
    "\n",
    "Now at this stump for selecting next step of the model that particular mis-classification needs to be resolved first. However, the say of this model is lower than the say of the first stump. \n",
    "\n",
    "Remember Say is given by:\n",
    "$\\frac{1}{2}\\times log(\\large \\frac{1-Total Error}{Total Error})$\n",
    "\n",
    "Now here total error becomes 0.07 because there is one record with 0.07 weight. So the say at this stump becomes:\n",
    "\n",
    "$\\frac{1}{2}\\times log(\\large \\frac{1-0.07}{0.07})$ = 1.293\n",
    "\n",
    "Now we will recompute the weights and give higher weight to the misclassified record and lower weight to correctly classified. \n",
    "\n",
    "We repeat the step till we arrive at the final decision. \n",
    "\n",
    "Our algorithm will be serial-processing of all the above steps to correctly predict the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4ee66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
